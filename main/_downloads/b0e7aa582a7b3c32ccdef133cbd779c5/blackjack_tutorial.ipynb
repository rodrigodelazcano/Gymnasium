{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Solving Blackjack with Q-Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"file://_static/img/tutorials/blackjack_AE_loop.jpg\" width=\"650\" alt=\"agent-environment-diagram\">\n\nIn this tutorial, we\u2019ll explore and solve the *Blackjack-v1*\nenvironment.\n\n**Blackjack** is one of the most popular casino card games that is also\ninfamous for being beatable under certain conditions. This version of\nthe game uses an infinite deck (we draw the cards with replacement), so\ncounting cards won\u2019t be a viable strategy in our simulated game.\n\n**Objective**: To win, your card sum should be greater than than the\ndealers without exceeding 21.\n\n**Approach**: To solve this environment by yourself, you can pick your\nfavorite discrete RL algorithm. The presented solution uses *Q-learning*\n(a model-free RL algorithm).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Environment Setup\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Till Zemann\n# License: MIT License\n\nfrom collections import defaultdict\n\nimport gym\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\n\n# Let's start by creating the blackjack environment.\n# Note: We are going to follow the rules from Sutton & Barto.\n# Other versions of the game can be found below for you to experiment.\n\nenv = gym.make(\"Blackjack-v1\", sab=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. code:: py\n\n  # Other possible environment configurations:\n\n  env = gym.make('Blackjack-v1', natural=True, sab=False)``\n\n  env = gym.make('Blackjack-v1', natural=False, sab=False)``\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observing the environment\n\nFirst of all, we call ``env.reset()`` to start an episode. This function\nresets the environment to a starting position and returns an initial\n``observation``. We usually also set ``done = False``. This variable\nwill be useful later to check if a game is terminated. In this tutorial\nwe will use the terms observation and state synonymously but in more\ncomplex problems a state might differ from the observation it is based\non.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# reset the environment to get the first observation\ndone = False\nobservation, info = env.reset()\n\nprint(observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that our observation is a 3-tuple consisting of 3 discrete values:\n\n-  The players current sum\n-  Value of the dealers face-up card\n-  Boolean whether the player holds a usable ace (An ace is usable if it\n   counts as 11 without busting)\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executing an action\n\nAfter receiving our first observation, we are only going to use the\n``env.step(action)`` function to interact with the environment. This\nfunction takes an action as input and executes it in the environment.\nBecause that action changes the state of the environment, it returns\nfour useful variables to us. These are:\n\n-  ``next_state``: This is the observation that the agent will receive\n   after taking the action.\n-  ``reward``: This is the reward that the agent will receive after\n   taking the action.\n-  ``terminated``: This is a boolean variable that indicates whether or\n   not the episode is over.\n-  ``truncated``: This is a boolean variable that also indicates whether\n   the episode ended by early truncation.\n-  ``info``: This is a dictionary that might contain additional\n   information about the environment.\n\nThe ``next_state``, ``reward``, and ``done`` variables are\nself-explanatory, but the ``info`` variable requires some additional\nexplanation. This variable contains a dictionary that might have some\nextra information about the environment, but in the Blackjack-v1\nenvironment you can ignore it. For example in Atari environments the\ninfo dictionary has a ``ale.lives`` key that tells us how many lives the\nagent has left. If the agent has 0 lives, then the episode is over.\n\nBlackjack-v1 doesn\u2019t have a ``env.render()`` function to render the\nenvironment, but in other environments you can use this function to\nwatch the agent play. Important to note is that using ``env.render()``\nis optional - the environment is going to work even if you don\u2019t render\nit, but it can be helpful to see an episode rendered out to get an idea\nof how the current policy behaves. Note that it is not a good idea to\ncall this function in your training loop because rendering slows down\ntraining by a lot. Rather try to build an extra loop to evaluate and\nshowcase the agent after training.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sample a random action from all valid actions\naction = env.action_space.sample()\n\n# execute the action in our environment and receive infos from the environment\nobservation, reward, terminated, truncated, info = env.step(action)\n\nprint(\"observation:\", observation)\nprint(\"reward:\", reward)\nprint(\"terminated:\", terminated)\nprint(\"truncated:\", truncated)\nprint(\"info:\", info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once ``terminated = True`` or ``truncated=True``, we should stop the\ncurrent episode and begin a new one with ``env.reset()``. If you\ncontinue executing act`ons without resetting the environment, it still\nresponds but the output won\u2019t be useful for training (it might even be\nharmful if the agent learns on invalid data).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building an agent\n\nLet\u2019s build a ``Q-learning agent`` to solve *Blackjack-v1*! We\u2019ll need\nsome functions for picking an action and updating the agents action\nvalues. To ensure that the agents expores the environment, one possible\nsolution is the ``epsilon-greedy`` strategy, where we pick a random\naction with the percentage ``epsilon`` and the greedy action (currently\nvalued as the best) ``1 - epsilon``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class BlackjackAgent:\n    def __init__(self, lr=1e-3, epsilon=0.1, epsilon_decay=1e-4):\n        \"\"\"\n        Initialize an Reinforcement Learning agent with an empty dictionary\n        of state-action values (q_values), a learning rate and an epsilon.\n        \"\"\"\n        self.q_values = defaultdict(\n            lambda: np.zeros(env.action_space.n)\n        )  # maps a state to action values\n        self.lr = lr\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n\n    def get_action(self, state):\n        \"\"\"\n        Returns the best action with probability (1 - epsilon)\n        and a random action with probability epsilon to ensure exploration.\n        \"\"\"\n        # with probability epsilon return a random action to explore the environment\n        if np.random.random() < self.epsilon:\n            action = env.action_space.sample()\n\n        # with probability (1 - epsilon) act greedily (exploit)\n        else:\n            action = np.argmax(self.q_values[state])\n        return action\n\n    def update(self, state, action, reward, next_state, done):\n        \"\"\"\n        Updates the Q-value of an action.\n        \"\"\"\n        old_q_value = self.q_values[state][action]\n        max_future_q = np.max(self.q_values[next_state])\n        target = reward + self.lr * max_future_q * (1 - done)\n        self.q_values[state][action] = (1 - self.lr) * old_q_value + self.lr * target\n\n    def decay_epsilon(self):\n        self.epsilon = self.epsilon - epsilon_decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train the agent, we will let the agent play one episode (one complete\ngame is called an episode) at a time and then update it\u2019s Q-values after\neach episode. The agent will have to experience a lot of episodes to\nexplore the environment sufficiently.\n\nNow we should be ready to build the training loop.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# hyperparameters\nlearning_rate = 1e-3\nstart_epsilon = 0.8\nn_episodes = 200_000\nepsilon_decay = start_epsilon / n_episodes  # less exploration over time\n\nagent = BlackjackAgent(\n    lr=learning_rate, epsilon=start_epsilon, epsilon_decay=epsilon_decay\n)\n\n\ndef train(agent, n_episodes):\n    for episode in range(n_episodes):\n\n        # reset the environment\n        state, info = env.reset()\n        done = False\n\n        # play one episode\n        while not done:\n            action = agent.get_action(observation)\n            next_state, reward, terminated, truncated, info = env.step(action)\n            done = (\n                terminated or truncated\n            )  # if the episode terminated or was truncated early, set done to True\n            agent.update(state, action, reward, next_state, done)\n            state = next_state\n\n        agent.update(state, action, reward, next_state, done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, let\u2019s train!\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train(agent, n_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the results\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_grids(agent, usable_ace=False):\n\n    # convert our state-action values to state values\n    # and build a policy dictionary that maps observations to actions\n    V = defaultdict(float)\n    policy = defaultdict(int)\n    for obs, action_values in agent.q_values.items():\n        V[obs] = np.max(action_values)\n        policy[obs] = np.argmax(action_values)\n\n    X, Y = np.meshgrid(\n        np.arange(12, 22), np.arange(1, 11)  # players count\n    )  # dealers face-up card\n\n    # create the value grid for plotting\n    Z = np.apply_along_axis(\n        lambda obs: V[(obs[0], obs[1], usable_ace)], axis=2, arr=np.dstack([X, Y])\n    )\n    value_grid = X, Y, Z\n\n    # create the policy grid for plotting\n    policy_grid = np.apply_along_axis(\n        lambda obs: policy[(obs[0], obs[1], usable_ace)], axis=2, arr=np.dstack([X, Y])\n    )\n    return value_grid, policy_grid\n\n\ndef create_plots(value_grid, policy_grid, title=\"N/A\"):\n\n    # create a new figure with 2 subplots (left: state values, right: policy)\n    X, Y, Z = value_grid\n    fig = plt.figure(figsize=plt.figaspect(0.4))\n    fig.suptitle(title, fontsize=16)\n\n    # plot the state values\n    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n    ax1.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=\"viridis\", edgecolor=\"none\")\n    plt.xticks(range(12, 22), range(12, 22))\n    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n    ax1.set_title(\"State values: \" + title)\n    ax1.set_xlabel(\"Player sum\")\n    ax1.set_ylabel(\"Dealer showing\")\n    ax1.zaxis.set_rotate_label(False)\n    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n    ax1.view_init(20, 220)\n\n    # plot the policy\n    fig.add_subplot(1, 2, 2)\n    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n    ax2.set_title(\"Policy: \" + title)\n    ax2.set_xlabel(\"Player sum\")\n    ax2.set_ylabel(\"Dealer showing\")\n    ax2.set_xticklabels(range(12, 22))\n    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n\n    # add a legend\n    legend_elements = [\n        Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n        Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\"),\n    ]\n    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n    return fig\n\n\n# state values & policy with usable ace (ace counts as 11)\nvalue_grid, policy_grid = create_grids(agent, usable_ace=True)\nfig1 = create_plots(value_grid, policy_grid, title=\"With usable ace\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"file://_static/img/tutorials/blackjack_with_usable_ace.png\">\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# state values & policy without usable ace (ace counts as 1)\nvalue_grid, policy_grid = create_grids(agent, usable_ace=False)\nfig2 = create_plots(value_grid, policy_grid, title=\"Without usable ace\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"file://_static/img/tutorials/blackjack_without_usable_ace.png\">\n\nIt's good practice to call env.close() at the end of your script,\nso that any used resources by the environment will be closed.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hopefully this Tutorial helped you get a grip of how to interact with\nOpenAI-Gym environments and sets you on a journey to solve many more RL\nchallenges.\n\nIt is recommended that you solve this environment by yourself (project\nbased learning is really effective!). You can apply your favorite\ndiscrete RL algorithm or give Monte Carlo ES a try (covered in [Sutton &\nBarto](http://incompleteideas.net/book/the-book-2nd.html)_, section\n5.3) - this way you can compare your results directly to the book.\n\nBest of fun!\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}